{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import langdetect\n",
    "from tqdm import tqdm\n",
    "from nltk.corpus import stopwords\n",
    "from langdetect import detect\n",
    "from textblob import TextBlob\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/xuenichen/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/xuenichen/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/xuenichen/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "autism_file_path = '/Users/xuenichen/Desktop/BEF_Chen/dataset/Twitter Autism/autism.csv'\n",
    "control_file_path = '/Users/xuenichen/Desktop/BEF_Chen/dataset/Twitter Autism/control_group.csv'\n",
    "\n",
    "# Load the datasets\n",
    "autism_df = pd.read_csv(autism_file_path)\n",
    "control_df = pd.read_csv(control_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New total number of rows: 3137952\n",
      "New total number of rows: 3377518\n"
     ]
    }
   ],
   "source": [
    "print(\"New total number of rows:\", autism_df.shape[0])\n",
    "print(\"New total number of rows:\", control_df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows where 'User_ID' is NaN in autism_df\n",
    "autism_df = autism_df.dropna(subset=['User_ID'])\n",
    "\n",
    "# Drop rows where 'User_ID' is NaN in control_df\n",
    "control_df = control_df.dropna(subset=['User_ID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User_ID</th>\n",
       "      <th>Friends count</th>\n",
       "      <th>Followers count</th>\n",
       "      <th>Tweet date</th>\n",
       "      <th>Tweet id</th>\n",
       "      <th>Language</th>\n",
       "      <th>Tweet text</th>\n",
       "      <th>Hashtags</th>\n",
       "      <th>Location</th>\n",
       "      <th>Reply count</th>\n",
       "      <th>Retweet count</th>\n",
       "      <th>Like count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>b0b63865cbb84efcd5422e42d13c8672707c82185c2cdf...</td>\n",
       "      <td>115</td>\n",
       "      <td>1175</td>\n",
       "      <td>2020-12-31 23:59:58+00:00</td>\n",
       "      <td>1344795463317389312</td>\n",
       "      <td>en</td>\n",
       "      <td>People come and go, yet they remain irreplacea...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>978a4548ee471aa84938dd67052799fa586d45fe405517...</td>\n",
       "      <td>2955</td>\n",
       "      <td>3053</td>\n",
       "      <td>2016-03-05 23:59:54+00:00</td>\n",
       "      <td>706268029634613248</td>\n",
       "      <td>en</td>\n",
       "      <td>Wind 1.2 mph WNW. Barometer 1013.4 hPa, Rising...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Dorset, England</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5fbafd883c78ace9e30bccaa0caa350e797c625579aeba...</td>\n",
       "      <td>207</td>\n",
       "      <td>9938</td>\n",
       "      <td>2014-08-12 23:59:55+00:00</td>\n",
       "      <td>499344560506433536</td>\n",
       "      <td>en</td>\n",
       "      <td>â˜†*:ï½¡*:*ï½¡:*â˜†\\nluke hemmings &amp;amp; michael cliff...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>b8554e9a8fcc0eaf7485bfeb3ecc0d64373982d33f1660...</td>\n",
       "      <td>113</td>\n",
       "      <td>195</td>\n",
       "      <td>2021-03-04 23:59:53+00:00</td>\n",
       "      <td>1367625880030257155</td>\n",
       "      <td>en</td>\n",
       "      <td>I have a mud mask on and my Face ID wonâ€™t work ðŸ˜‚</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>979462d73dec55949483575709190ed03c0e9b79bbc7f6...</td>\n",
       "      <td>605</td>\n",
       "      <td>4912</td>\n",
       "      <td>2019-03-30 23:59:35+00:00</td>\n",
       "      <td>1112142359138050048</td>\n",
       "      <td>ht</td>\n",
       "      <td>voy a dormir 3h ****</td>\n",
       "      <td>NaN</td>\n",
       "      <td>icon @hanavbara</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             User_ID  Friends count  \\\n",
       "0  b0b63865cbb84efcd5422e42d13c8672707c82185c2cdf...            115   \n",
       "1  978a4548ee471aa84938dd67052799fa586d45fe405517...           2955   \n",
       "2  5fbafd883c78ace9e30bccaa0caa350e797c625579aeba...            207   \n",
       "3  b8554e9a8fcc0eaf7485bfeb3ecc0d64373982d33f1660...            113   \n",
       "4  979462d73dec55949483575709190ed03c0e9b79bbc7f6...            605   \n",
       "\n",
       "   Followers count                 Tweet date             Tweet id Language  \\\n",
       "0             1175  2020-12-31 23:59:58+00:00  1344795463317389312       en   \n",
       "1             3053  2016-03-05 23:59:54+00:00   706268029634613248       en   \n",
       "2             9938  2014-08-12 23:59:55+00:00   499344560506433536       en   \n",
       "3              195  2021-03-04 23:59:53+00:00  1367625880030257155       en   \n",
       "4             4912  2019-03-30 23:59:35+00:00  1112142359138050048       ht   \n",
       "\n",
       "                                          Tweet text Hashtags  \\\n",
       "0  People come and go, yet they remain irreplacea...      NaN   \n",
       "1  Wind 1.2 mph WNW. Barometer 1013.4 hPa, Rising...      NaN   \n",
       "2  â˜†*:ï½¡*:*ï½¡:*â˜†\\nluke hemmings &amp; michael cliff...      NaN   \n",
       "3   I have a mud mask on and my Face ID wonâ€™t work ðŸ˜‚      NaN   \n",
       "4                               voy a dormir 3h ****      NaN   \n",
       "\n",
       "          Location  Reply count  Retweet count  Like count  \n",
       "0              NaN            0              0           0  \n",
       "1  Dorset, England            0              0           0  \n",
       "2              NaN            0              1           0  \n",
       "3              NaN            0              0           0  \n",
       "4  icon @hanavbara            0              0           1  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "control_df = control_df.drop(\n",
    "    columns=['Source', 'Account created', 'Profile description'])\n",
    "autism_df = autism_df.drop(\n",
    "    columns=['Source', 'Account created', 'Profile description'])\n",
    "autism_df.head()\n",
    "control_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New total number of rows: 3137952\n",
      "New total number of rows: 3377518\n"
     ]
    }
   ],
   "source": [
    "print(\"New total number of rows:\", autism_df.shape[0])\n",
    "print(\"New total number of rows:\", control_df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows that have missing values in the 'Hashtags' column\n",
    "control_df = control_df.dropna(subset=['Hashtags']).copy()\n",
    "autism_df = autism_df.dropna(subset=['Hashtags']).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New total number of rows: 357015\n",
      "New total number of rows: 976592\n"
     ]
    }
   ],
   "source": [
    "print(\"New total number of rows:\", control_df.shape[0])\n",
    "print(\"New total number of rows:\", autism_df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Language ratio before dropping non-English rows:\n",
      " Language\n",
      "en     0.989040\n",
      "es     0.002944\n",
      "tl     0.002101\n",
      "fr     0.001605\n",
      "pt     0.001090\n",
      "in     0.000692\n",
      "nl     0.000431\n",
      "ht     0.000398\n",
      "it     0.000286\n",
      "de     0.000238\n",
      "et     0.000207\n",
      "vi     0.000151\n",
      "sv     0.000115\n",
      "no     0.000112\n",
      "da     0.000109\n",
      "pl     0.000098\n",
      "tr     0.000095\n",
      "eu     0.000087\n",
      "cy     0.000048\n",
      "ca     0.000045\n",
      "qme    0.000034\n",
      "lv     0.000025\n",
      "und    0.000011\n",
      "hi     0.000008\n",
      "lt     0.000008\n",
      "hu     0.000008\n",
      "is     0.000006\n",
      "cs     0.000006\n",
      "ja     0.000003\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "control_df_ratio = control_df['Language'].value_counts(normalize=True)\n",
    "print(\"Language ratio before dropping non-English rows:\\n\", control_df_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Language ratio before dropping non-English rows:\n",
      " Language\n",
      "en     0.998064\n",
      "fr     0.000620\n",
      "de     0.000191\n",
      "nl     0.000186\n",
      "tl     0.000167\n",
      "es     0.000152\n",
      "ht     0.000081\n",
      "in     0.000073\n",
      "pt     0.000072\n",
      "et     0.000069\n",
      "da     0.000065\n",
      "no     0.000059\n",
      "sv     0.000034\n",
      "it     0.000034\n",
      "tr     0.000032\n",
      "ca     0.000018\n",
      "cy     0.000018\n",
      "vi     0.000013\n",
      "pl     0.000010\n",
      "lv     0.000010\n",
      "eu     0.000010\n",
      "is     0.000007\n",
      "cs     0.000005\n",
      "und    0.000003\n",
      "hi     0.000002\n",
      "lt     0.000002\n",
      "qme    0.000002\n",
      "hu     0.000001\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "autism_df_ratio = autism_df['Language'].value_counts(normalize=True)\n",
    "print(\"Language ratio before dropping non-English rows:\\n\", autism_df_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "control_df = control_df[control_df['Language'] == 'en']\n",
    "autism_df = autism_df[autism_df['Language'] == 'en']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New total number of rows: 974701\n",
      "New total number of rows: 353102\n"
     ]
    }
   ],
   "source": [
    "print(\"New total number of rows:\", autism_df.shape[0])\n",
    "print(\"New total number of rows:\", control_df.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# clean the tweet texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to clean tweet text\n",
    "def clean_tweet_text(text):\n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "    # Remove user @ references and '#' from tweet\n",
    "    text = re.sub(r'\\@\\w+|\\#', '', text)\n",
    "    # Remove HTML entities\n",
    "    text = re.sub(r'&[a-z]+;', '', text)\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Tokenization\n",
    "    tokens = word_tokenize(text)\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_tokens = [word for word in tokens if word not in stop_words]\n",
    "    # Lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_text = [lemmatizer.lemmatize(word) for word in filtered_tokens]\n",
    "    # Rejoin words\n",
    "    clean_text = ' '.join(lemmatized_text)\n",
    "    return clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the cleaning function to the 'Tweet text' column\n",
    "control_df['Tweet text'] = control_df['Tweet text'].apply(clean_tweet_text)\n",
    "\n",
    "autism_df['Tweet text'] = autism_df['Tweet text'].apply(clean_tweet_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "control_df = control_df.drop(columns=['Language'])\n",
    "autism_df = autism_df.drop(columns=['Language'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the pipeline\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"cardiffnlp/twitter-roberta-base-sentiment\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"cardiffnlp/twitter-roberta-base-sentiment\")\n",
    "sentiment_pipeline = pipeline(\n",
    "    \"sentiment-analysis\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "\n",
    "# Function to convert sentiment analysis output to continuous scale, with rounding\n",
    "def get_continuous_sentiment(text, pipeline):\n",
    "    result = pipeline(text)[0]\n",
    "    # The output is a dictionary with 'label' and 'score'.\n",
    "    # Labels correspond to 'LABEL_0' (negative), 'LABEL_1' (neutral), 'LABEL_2' (positive)\n",
    "    label = result['label']\n",
    "    if label == 'LABEL_2':  # Positive\n",
    "        return 2\n",
    "    elif label == 'LABEL_0':  # Negative\n",
    "        return -2\n",
    "    else:  # Neutral\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_get_continuous_sentiment(text, sentiment_pipeline):\n",
    "    # Check if text is a non-empty string and not NaN\n",
    "    if pd.notnull(text) and text.strip():\n",
    "        return get_continuous_sentiment(text, sentiment_pipeline)\n",
    "    else:\n",
    "        return 0  # Default sentiment value for invalid inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 353102/353102 [4:16:12<00:00, 22.97it/s]  \n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 974701/974701 [12:03:12<00:00, 22.46it/s]   \n"
     ]
    }
   ],
   "source": [
    "tqdm.pandas()\n",
    "control_df['Tweet Text Sentiment'] = control_df['Tweet text'].progress_apply(\n",
    "    lambda x: safe_get_continuous_sentiment(x, sentiment_pipeline))\n",
    "autism_df['Tweet Text Sentiment'] = autism_df['Tweet text'].progress_apply(\n",
    "    lambda x: safe_get_continuous_sentiment(x, sentiment_pipeline))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment analysis with continuous scaling completed.\n",
      "Sentiment analysis with continuous scaling completed.\n"
     ]
    }
   ],
   "source": [
    "# Save the modified DataFrame\n",
    "control_df.to_csv(\n",
    "    '/Users/xuenichen/Desktop/BEF_Chen/dataset/control_tweets_with_sentiment-all.csv', index=False)\n",
    "\n",
    "print(\"Sentiment analysis with continuous scaling completed.\")\n",
    "\n",
    "\n",
    "autism_df.to_csv(\n",
    "    '/Users/xuenichen/Desktop/BEF_Chen/dataset/austim_tweets_with_sentiment-all.csv', index=False)\n",
    "\n",
    "print(\"Sentiment analysis with continuous scaling completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# save the files with dropping rows with unique tweets, as no intractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_id_frequencies = control_df['Tweet id'].value_counts()\n",
    "\n",
    "# Step 2: Filter to keep only rows with 'Tweet id' that appear more than once\n",
    "control_df_inter = control_df[control_df['Tweet id'].map(tweet_id_frequencies) > 1]\n",
    "\n",
    "tweet_id_frequencies = autism_df['Tweet id'].value_counts()\n",
    "\n",
    "# Step 2: Filter to keep only rows with 'Tweet id' that appear more than once\n",
    "autism_df_inter = autism_df[autism_df['Tweet id'].map(tweet_id_frequencies) > 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New total number of rows: 30\n",
      "New total number of rows: 1978\n"
     ]
    }
   ],
   "source": [
    "print(\"New total number of rows:\", autism_df_inter.shape[0])\n",
    "print(\"New total number of rows:\", control_df_inter.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment analysis with continuous scaling completed.\n",
      "Sentiment analysis with continuous scaling completed.\n"
     ]
    }
   ],
   "source": [
    "# Save the modified DataFrame\n",
    "control_df_inter.to_csv(\n",
    "    '/Users/xuenichen/Desktop/BEF_Chen/dataset/control_tweets_with_sentiment-inter.csv', index=False)\n",
    "\n",
    "print(\"Sentiment analysis with continuous scaling completed.\")\n",
    "\n",
    "\n",
    "autism_df_inter.to_csv(\n",
    "    '/Users/xuenichen/Desktop/BEF_Chen/dataset/austim_tweets_with_sentiment-inter.csv', index=False)\n",
    "\n",
    "print(\"Sentiment analysis with continuous scaling completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# save files with dropping rows with no value for 'reply count'& 'retweet cont'+  drop rows with no value for 'friend count'& 'follower cont'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out rows where 'Reply count', 'Retweet count' are all 0\n",
    "control_df_act = control_df[((control_df['Reply count'].notna() & control_df['Reply count'] != 0) |\n",
    "                         (control_df['Retweet count'].notna() & control_df['Retweet count'] != 0))]\n",
    "\n",
    "# Filter out rows where 'Friends count', 'Followers count' are both 0\n",
    "control_df_act = control_df[((control_df['Friends count'].notna() & control_df['Friends count'] != 0) |\n",
    "                         (control_df['Followers count'].notna() & control_df['Followers count'] != 0))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out rows where 'Reply count', 'Retweet count' are all 0\n",
    "autism_df_act = autism_df[((autism_df['Reply count'].notna() & autism_df['Reply count'] != 0) |\n",
    "                       (autism_df['Retweet count'].notna() & autism_df['Retweet count'] != 0))]\n",
    "\n",
    "# Filter out rows where 'Friends count', 'Followers count' are both 0\n",
    "autism_df_act = autism_df[((autism_df['Friends count'].notna() & autism_df['Friends count'] != 0) |\n",
    "                       (autism_df['Followers count'].notna() & autism_df['Followers count'] != 0))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New total number of rows: 724418\n",
      "New total number of rows: 267897\n"
     ]
    }
   ],
   "source": [
    "print(\"New total number of rows:\", autism_df_act.shape[0])\n",
    "print(\"New total number of rows:\", control_df_act.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicates based on 'Tweet text'\n",
    "autism_df_act = autism_df_act.drop_duplicates(subset=['Tweet text'])\n",
    "control_df_act = control_df_act.drop_duplicates(subset=['Tweet text'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
